---
layout: post
title:  "Convolution for language?"
date:   2020-02-25 00:04:04 -0500
categories: jekyll update
---

The topic of the day: Convolutional NN features with its convolution operation. `tf.nn.conv2d`

**Filters:** (aka the *f* ) multiple different matrix as imagine edge detectors for example ðŸ•µ. Learning what is going on in an imagine by learning the parameters through filter matrixes is more efficient than hand-coding all the filters (which is almost impossible, if possible it's going to be a long long time). *f* is an `odd number` by convention.

**Side-effect of filters**: Regular convolution will always shrink the output. Throwing away a lot of information from the edge of the imagine. The solution is padding **Padding** (aka the *p*)

**The input shape after padding**: (n+2p-f+1) * (n+2p-f+1), if the previous output dimension is (n-f+1) * (n-f+1).

**How to pad?** `Valid` and `Same` make n+2p-f+1 = n. The result is p=(f-1)/2.

I learned my knowledge from [Deeplearning.ai] by Andrew Ng.

[Deeplearning.ai]:https://www.youtube.com/watch?v=smHa2442Ah4&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=4
