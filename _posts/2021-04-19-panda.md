---
layout: post
title: panda and NLTK
date: Â  2021-04-19 5:10 -0500
categories: jekyll update
---
*last update on 04262021*

1) **pd.Series** is a different data structure from **pd.DataFrame**: A Serie can be seeing as one column of a DataFrame.(`pf.DaraFramename['column_name']` is a serie) DataFrames can store a collection of Series.

pd.Series example:

|Index |Column_name|
|1    |4-13-82 Other Child Mental Health Outcomes Sca...
|2     |9/22/89 CPT Code: 90792: With medical services\n
|3      | 9/02/76 CPT Code: 90791: No medical services\n
|4       |                        9/12/71 [report_end]\n
|5    |10/24/86 Communication with referring physicia...
|6     |  03/31/1985 Total time of visit (in minutes):\n

pd.DataFrame example:

|Index | Match Index |column
134	|0	  |14 Jan 1981
135	|0	|  10 Oct 1985
136	|0	  |11 February 1985
137	|0	 | 10 Feb 1983
138	|0	  |05 Feb 1992
139	|0	  |21 Oct 2012
140	|0	  |14 Feb 1995

```python
#regex Series tring replacement
>>>pd.Series(['aaaa', 'abbb', np.nan]).str.replace('a.', 'zzz')

0    zzzzzz
1     zzzbb
2       NaN
dtype: object

```
The returned data structure of `Series.str.extract()` and `Series.str.extractall()` can be different. `Series.str.extractall()` always return DataFrame. `Series.str.extract()` if `expand = True` (the default value), the returned data structure can be DataFrame, Series, or Indexes.

2) **Stemming** is different from **lemmatization**: the former one (for example in NLTK, porter_stemmer) can have non-words left after trimming, while the later one (for example in NLTK, WordNetLemmatizer) ensures the remained part are valid words.

`nltk.word_tokenizer` tokenizes English auxiliary and negation contraction apart:

```python
# auxiliaries and negation contraction apart
>>> text_bri = "We can't and shouldn't waste food."
>>> nltk.word_tokenize(text_bri)
['We', 'ca', "n't", 'and', 'should', "n't", 'waste', 'food', '.']

```

Regex exercise: capture different formatted dates.

```python
# various format
#04/20/2009; 04/20/09; 4/20/09; 4/3/09
#Mar-20-2009; Mar 20, 2009; March 20, 2009; Mar. 20, 2009; Mar 20 2009;
#20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009
#Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009
#Feb 2009; Sep 2009; Oct 2010
#6/2008; 12/2009
#2009; 2010

#return a pandas Series in chronological order
#For input:
#0    1999
#1    2010
#2    1978
#3    2015
#4    1985

#Output:
#0    2
#1    4
#2    0
#3    1
#4    3


def date_sorter():
  date_from_word = df.str.extract(r'((?:\d{,2}\s)?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*(?:\W)\s?\d{,2}[a-z]*(?:\W)?\s?\d{2,4})', expand=False)
  date_from_number = df.str.extract(r'((?:\d{1,2})(?:(?:\/|-)\d{1,2})(?:(?:\/|-)\d{2,4}))', expand=False)
  date_wo_day = df.str.extract(r'((?:\d{1,2}(?:-|\/))?\d{4})', expand=False)
  dates = pd.to_datetime(date_from_word.fillna(date_from_number).fillna(date_wo_day).replace('Decemeber','December', regex = True).replace('Janaury','January',regex = True))
  final = pd.Series(dates.sort_values())
  final_index = list(final.index)
  return pd.Series(final_index)

```

3) nltk.FreqDist returns a dictionary (unordered); return nltk.FreqDist(text1).most_common(20) returns a list of tuples. Yet, Text(nltk format).vocab() will also return a list tuples of word and freq.

4) `operator.itemgetter(n)` so far seems to be equivalent to `lambda x: x[n]`.


Edit Distance (a.k.a. Levenshtein Distance)
Jaccard Distance  
